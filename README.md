---
title: RAG Chatbot â€” Streamlit
emoji: ğŸ§ 
colorFrom: purple
colorTo: blue
sdk: streamlit
app_file: app.py
pinned: false
---

# ğŸ¤– RAG Chatbot â€” Streamlit (HF Spaces) 

**Live demo:** https://huggingface.co/spaces/momalmir/rag-chatbot-doc-qa  
**Stack:** Streamlit â€¢ LangChain â€¢ FAISS â€¢ sentence-transformers â€¢ BM25 â€¢ MMR â€¢ Crossâ€‘encoder reâ€‘ranker â€¢ OpenRouter (optional)

This project is a **documentâ€‘questionâ€‘answering** app that lets users upload files, build a temporary or shared index, and **ask grounded questions**. It runs as a **Streamlit** app on **Hugging Face Spaces** and can also be used locally.

[![ğŸ¤— Spaces](https://img.shields.io/badge/ğŸ¤—%20Spaces-Deployed-blue)](https://huggingface.co/spaces/momalmir/rag-chatbot-doc-qa)

---

![Demo](assets/demo.gif)



## ğŸ§­ Twoâ€‘Mode RAG (domainâ€‘agnostic)

### 1) **Evidenceâ€‘only**
- Returns **topâ€‘matching sentences** with inline **`[n]` citations** (no LLM).  
- Retrieval pipeline:
  - **BM25** (lexical) builds a corpus over all chunks and gives sparse scores.
  - **FAISS** (vector) does semantic ANN search using Hugging Face embeddings.
  - **MMR** (**m**aximal **m**arginal **r**elevance) on the FAISS retriever for diversity.
  - **Optional Crossâ€‘encoder reâ€‘rank** (`BAAI/bge-reranker-base`) for precision at topâ€‘k.
- Output = **bulleted evidence list** (up to `Max bullets`) with `[n]` mapping to numbered contexts.

### 2) **Summarize with OpenRouter**
- Uses an LLM (via OpenRouter) to generate **2â€“4 sentences** that **only summarize the evidence bullets**, preserving `[n]` citations.
- If the API key is missing or the call fails, the app cleanly **falls back** to Evidenceâ€‘only mode.
- Great for compact, interviewâ€‘ready answers that remain **grounded** in retrieved text.

---

## ğŸ” Hybrid Retrieval (BM25 + FAISS + MMR)

```
Question â†’ BM25 over chunks  â†’ top lexical docs
         â†’ FAISS (MMR)       â†’ top semantic docs (diverse)
         â†’ Merge + dedupe    â†’ candidate set
         â†’ Crossâ€‘encoder     â†’ rerank for precision (optional)
         â†’ Keep top final_k  â†’ contexts
```

- **BM25** (`rank_bm25`) excels at **exact term matching**, headers, and tabular text.
- **FAISS** + Hugging Face embeddings excels at **semantic similarity**.
- **MMR** (via `as_retriever(search_type="mmr")`) reduces nearâ€‘duplicates and improves coverage.
- **Crossâ€‘encoder** (bgeâ€‘rerankerâ€‘base) reads **(query, passage)** pairs and reâ€‘scores them for **precision@k**.

> UI knobs map to this pipeline: `initial_k` (candidate breadth), `final_k` (kept contexts), `Use crossâ€‘encoder reranker` (on/off).

---

## ğŸ”— Citation Mechanism: `[n]`

- We build a **numbered context block**:
  ```
  Context [1]: ... (sentences clipped for question)
  Context [2]: ...
  ```
- Evidence bullets end with `[n]` where `n` is the **context number** the sentence came from.  
- The UI also shows a **citation panel** with entries like
  - `filename.pdf (p.3)` or `notes.md` (page optional)
- In **Summarize mode**, the prompt **requires the LLM to keep `[n]`** in the final text and avoid inventing citations.

### Why this design?
- `[n]` is languageâ€‘agnostic, compact, and easy to render in Streamlit.
- It keeps the answer auditable: users can jump to **Context [n]** and verify.

---

## ğŸ§± Text & Sentence Handling

- **Normalization**: join hyphenated line breaks, collapse spaces/newlines, strip Markdown artifacts (`**`), etc.
- **Questionâ€‘aware filtering**: from each chunk we keep sentences that **share words** with the question (fallback to first sentences).
- **Junk filter**: drop very short fragments, allâ€‘caps headings, section labels, and symbol lines.
- **Topâ€‘sentences**: score individual sentences with **BM25** over sentences; keep diverse topâ€‘N bullets (deâ€‘dup by hash).

---

## âš™ï¸ Sidebar Controls (UI â†’ Internals)

| UI Control | Meaning | Internal |
|---|---|---|
| **Embedding model** | sentence-transformers encoder | `HuggingFaceEmbeddings(model_name=...)` |
| **initial_k** | retriever fanâ€‘out | FAISS `k` + BM25 topâ€‘K for hybrid merge |
| **final_k** | contexts kept | top `final_k` after (optional) reâ€‘rank |
| **Use crossâ€‘encoder reranker** | precision boost | `BAAI/bge-reranker-base` |
| **Max bullets** | evidence sentences | topâ€‘N via BM25(on sentences) |
| **chunk_size / chunk_overlap** | chunking strategy | affects recall and context integrity |
| **Index directory** | persistence option | `index/` (shared) or inâ€‘memory session |

---

## ğŸ› ï¸ Local Run

```bash
git clone https://github.com/<your-username>/RAG-Chatbot.git
cd RAG-Chatbot
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
streamlit run app.py
```

**.env (optional for summarize mode)**
```ini
OPENROUTER_API_KEY=your_key_here
```

---


## ğŸ§ª Usage Pattern

1) **Ingest Documents**: Drop files, set `chunk_size/overlap`, build **inâ€‘memory** index (perâ€‘session) or persist to `/index` (shared).  
2) **Ask a Question**: Switch tab, choose **Evidence** or **Summarize**, tune `initial_k/final_k`, toggle **reâ€‘ranker**.  
3) **Read result**: Bullets with `[n]` + citation panel; or a short LLMâ€‘based summary that preserves `[n]`.

---

## â“ Troubleshooting

- **Low recall** â†’ increase `initial_k`, reduce `chunk_size`, enable MMR.
- **Offâ€‘topic bullets** â†’ enable crossâ€‘encoder, lower `final_k`.
- **LLM fails** â†’ check `OPENROUTER_API_KEY`; fallback should show evidence bullets.
- **Scanned PDFs** â†’ OCR not included; preâ€‘OCR or add an OCR step.

---

## ğŸ“œ License & Credits

- MIT License
- sentence-transformers, FAISS, BAAI/bgeâ€‘rerankerâ€‘base, LangChain, Hugging Face Spaces.

---

## ğŸ‘¤ Author

**Mostafa Malmir (MoMalmir)**  
ğŸ”— HF Space: [huggingface.co/spaces/momalmir](https://huggingface.co/spaces/momalmir)  
ğŸ”— GitHub: [github.com/momalmir](https://github.com/momalmir)
ğŸ”— LinkedIn: [linkedin.com/in/mostafa-malmir](https://linkedin.com/in/mostafa-malmir)

